This is my personal project during the 2025 summer break.

This project uses PyTorch to implement an encoder-decoder Transformer from scratch (almost). 
The following components are implemented:
- Word embedding layer
- Learnable positional encoding layer
- Multihead Self/Cross-Attention layer
- Encoder
- Decoder

Currently, fewer than 200 pairs of dialog texts are used to train this model, to quickly see the model result. Therefore, the performance is not quite good temporarily.
I plan to switch to a larger and better dataset, and re-train the model later.
