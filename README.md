# Introduction

This is my personal project during the 2025 summer break.

This project uses PyTorch to implement an encoder-decoder Transformer from scratch (almost). 
The following components are implemented:
- Word embedding layer
- Learnable positional encoding layer
- Multihead Self/Cross-Attention layer
- Encoder
- Decoder

Currently, fewer than 200 pairs of dialog texts are used to train this model, to quickly see the model result. Therefore, the performance is not quite good temporarily.
I plan to switch to a larger and better dataset and re-train the model later.

## Current Artificial Idiot Performance
![image](https://github.com/user-attachments/assets/10f5f7d8-b94e-40de-9d27-eb2b5bbdc7db)
